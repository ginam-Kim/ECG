{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1SdyD14FrnHtuFcB+MsNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ginam-Kim/ECG/blob/main/helper_code%EC%84%A4%EB%AA%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFCQPLNsofZd",
        "outputId": "b9fd3c99-315d-48d0-a387-bae04d83f2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ECG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5fdl_wEouVV",
        "outputId": "d25454cc-c3f2-4f06-d530-4e2cebd56061"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ECG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rOvC4sHpkSN",
        "outputId": "45f907ee-621a-40f4-c670-be5245b4c65a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code  ecg-image-kit  python-example-2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd python-example-2024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7SNlw4aplNG",
        "outputId": "fe9102b4-674c-413b-e0fa-c7667fc3f2c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ECG/python-example-2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy-mmZkrppqY",
        "outputId": "90680f55-158f-44f1-839d-01ea41abe164"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add_image_filenames.py\tmodel\t\t       remove_hidden_data.py  train_model.py\n",
            "Dockerfile\t\tprepare_ptbxl_data.py  requirements.txt\n",
            "helper_code.py\t\t__pycache__\t       run_model.py\n",
            "LICENSE\t\t\tREADME.md\t       team_code.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "xRPYKVwWqSLQ",
        "outputId": "a1fae443-effe-47a4-ee81-f9f2425acb7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting joblib==1.3.2 (from -r requirements.txt (line 1))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.2 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.5.3 (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==9.3.0 (from -r requirements.txt (line 4))\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.3.2 (from -r requirements.txt (line 5))\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wfdb==4.1.2 (from -r requirements.txt (line 6))\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb==4.1.2->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb==4.1.2->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb==4.1.2->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb==4.1.2->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb==4.1.2->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb==4.1.2->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb==4.1.2->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb==4.1.2->-r requirements.txt (line 6)) (2024.2.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb==4.1.2->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb==4.1.2->-r requirements.txt (line 6)) (2.22)\n",
            "Installing collected packages: pillow, numpy, joblib, pandas, scikit-learn, wfdb\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.3.2 numpy-1.26.2 pandas-1.5.3 pillow-9.3.0 scikit-learn-1.3.2 wfdb-4.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pandas"
                ]
              },
              "id": "ee11f41ef6894fcc864fe73d5717fb5e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# helper_code.py"
      ],
      "metadata": {
        "id": "hg6xaEp-p9Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge data I/O functions\n",
        "- 데이터 입출력 함수"
      ],
      "metadata": {
        "id": "QECcHhyaxlCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Do *not* edit this script.\n",
        "# These are helper functions that you can use with your code.\n",
        "# Check the example code to see how to import these functions to your code.\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "### Challenge data I/O functions"
      ],
      "metadata": {
        "id": "HMWsVmrrrZK5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find_records\n",
        "- 특정 폴더 구조 내에서 .hea 파일을 찾고, 그 파일들의 이름을 정리하여 반환"
      ],
      "metadata": {
        "id": "gxnqEgORrcQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the records in a folder and its subfolders.\n",
        "def find_records(folder):\n",
        "    records = set()\n",
        "    for root, directories, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            extension = os.path.splitext(file)[1]\n",
        "            if extension == '.hea':\n",
        "                record = os.path.relpath(os.path.join(root, file), folder)[:-4]\n",
        "                records.add(record)\n",
        "    records = sorted(records)\n",
        "    return records"
      ],
      "metadata": {
        "id": "EB-M9PNyryeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제\n",
        "folder = '/path/to/directory'\n",
        "record_list = find_records(folder)\n",
        "print(record_list)"
      ],
      "metadata": {
        "id": "6evf-tW2rzSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_header\n",
        "- 특정 레코드의 정보를 포함하는 헤더 파일을 읽어올 때 유용"
      ],
      "metadata": {
        "id": "4xuQMxO-r7a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the header for a record.\n",
        "def load_header(record):\n",
        "    header_file = get_header_file(record)\n",
        "    header = load_text(header_file)\n",
        "    return header"
      ],
      "metadata": {
        "id": "FM3b7ahGr7GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제\n",
        "record = 'sample_record'\n",
        "header = load_header(record)\n",
        "print(header)"
      ],
      "metadata": {
        "id": "PgTV4S8BsL2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_signal\n",
        "- 웨이브폼 데이터베이스(WFDB) 형식의 신호 데이터를 읽어와서 분석하거나 처리하는 데 매우 유용\n",
        "- wfdb\n",
        "  - 생체 신호 데이터베이스(WFDB) 형식의 데이터를 읽고 쓰기 위한 파이썬 패키지\n",
        "  - wfdb.rdsamp(record) 함수는 주어진 레코드 이름에 해당하는 신호 데이터를 읽어오는데 사용"
      ],
      "metadata": {
        "id": "cE1s-45KsQ97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the signal(s) for a record.\n",
        "def load_signal(record):\n",
        "    import wfdb\n",
        "\n",
        "    signal_files = get_signal_files(record)\n",
        "    if signal_files:\n",
        "        signal, fields = wfdb.rdsamp(record)\n",
        "    else:\n",
        "        signal, fields = None, None\n",
        "    return signal, fields"
      ],
      "metadata": {
        "id": "vo0dets9sQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제\n",
        "record = 'sample_record'\n",
        "signal, fields = load_signal(record)\n",
        "print(signal)\n",
        "print(fields)"
      ],
      "metadata": {
        "id": "bLan4Yeds0BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_signals\n",
        "- load_signal과 동일\n",
        "- 더 명확하게 의도를 전달하기 위함"
      ],
      "metadata": {
        "id": "24k2QO1etI1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the signal(s) for a record.\n",
        "def load_signals(record):\n",
        "    return load_signal(record)"
      ],
      "metadata": {
        "id": "RtDesDIEtIH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_image\n",
        "- record 이름을 입력으로 받아, 해당 레코드의 이미지 파일을 찾고, 그 파일의 이미지 데이터를 로드하여 리스트로 반환하는 함수"
      ],
      "metadata": {
        "id": "_RvMuy3WtS8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image(s) for a record.\n",
        "def load_image(record):\n",
        "    from PIL import Image\n",
        "\n",
        "    path = os.path.split(record)[0]\n",
        "    image_files = get_image_files(record)\n",
        "\n",
        "    images = list()\n",
        "    for image_file in image_files:\n",
        "        image_file_path = os.path.join(path, image_file)\n",
        "        if os.path.isfile(image_file_path):\n",
        "            image = Image.open(image_file_path)\n",
        "            images.append(image)\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "iIBCIoXZtTon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제\n",
        "record = 'sample_record'\n",
        "images = load_image(record)\n",
        "for img in images:\n",
        "    img.show()"
      ],
      "metadata": {
        "id": "r00KO4b5txol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_images"
      ],
      "metadata": {
        "id": "urm6r6-Ot47G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image(s) for a record.\n",
        "def load_images(record):\n",
        "    return load_image(record)"
      ],
      "metadata": {
        "id": "KGeG8YOut0gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_dx\n",
        "- 헤더 파일 형식: 헤더 파일은 보통 레코드의 메타데이터와 진단 정보를 포함하는 텍스트 파일입니다.\n",
        "- load_header 함수는 이 파일을 읽어와 내용을 반환하고, get_dxs_from_header 함수는 그 내용을 파싱하여 필요한 진단 정보를 추출합니다."
      ],
      "metadata": {
        "id": "UN_mBfc2t-3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dx class(es) for a record.\n",
        "def load_dx(record):\n",
        "    header = load_header(record)\n",
        "    dx = get_dxs_from_header(header)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "Na3pagqut0dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제\n",
        "record = 'sample_record'\n",
        "dx_info = load_dx(record)\n",
        "print(dx_info)"
      ],
      "metadata": {
        "id": "4lqHocuSt0bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_dxs"
      ],
      "metadata": {
        "id": "jnKi4aSVulAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dxs(record):\n",
        "    return load_dx(record)"
      ],
      "metadata": {
        "id": "mg3mYuh_ukhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_header\n",
        "- get_header_file(record): record 이름을 입력받아 해당 레코드의 헤더 파일 경로를 반환하는 함수\n",
        "- save_text(header_file, header): 파일 경로와 내용을 입력받아 해당 파일에 내용을 저장하는 함수입니다. 이 함수는 파일을 열고 내용을 쓰는 로직이 포함될 것입니다."
      ],
      "metadata": {
        "id": "ZnBGq6xMuqRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the header for a record.\n",
        "def save_header(record, header):\n",
        "    header_file = get_header_file(record)\n",
        "    save_text(header_file, header)"
      ],
      "metadata": {
        "id": "gXhCvQ3YumEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시\n",
        "record = 'sample_record'\n",
        "header_content = '헤더 내용 예시'\n",
        "save_header(record, header_content)"
      ],
      "metadata": {
        "id": "Do6uqudbumAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_signal\n",
        "- record: 신호를 저장할 레코드 파일의 경로입니다.\n",
        "- signal: 저장할 신호 데이터입니다.\n",
        "- comments: 선택적 매개변수로, 레코드에 추가할 주석들을 담은 리스트입니다. 기본값은 빈 리스트입니다.\n",
        "\n",
        "1. load_header 함수를 통해 레코드의 헤더 정보를 불러옵니다.\n",
        "2. 레코드 파일 경로와 이름을 분리합니다.\n",
        "3. 헤더 정보에서 샘플링 주파수, 신호 형식, ADC 게인, 베이스라인, 신호 단위, 신호 이름 등을 추출합니다.\n",
        "4. 만약 모든 신호 형식이 '16'이라면, 신호를 int16 형식으로 클리핑하고 변환합니다.\n",
        "5. 그렇지 않으면, 해당하는 신호 형식이 아직 구현되지 않았음을 알리는 예외를 발생시킵니다.\n",
        "6. wfdb.wrsamp 함수를 사용하여 신호를 레코드로 기록합니다. 이 함수는 WFDB 라이브러리에 포함된 함수로, 신호 데이터와 관련된 여러 정보들을 기록합니다.\n",
        "\n",
        "\n",
        "※ 만약 신호 형식이 '16'이 아니라면 추가 구현이 필요"
      ],
      "metadata": {
        "id": "sTk62dxGvJsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the signal(s) for a record.\n",
        "def save_signal(record, signal, comments=list()):\n",
        "    header = load_header(record)\n",
        "    path, record = os.path.split(record)\n",
        "    sampling_frequency = get_sampling_frequency(header)\n",
        "    signal_formats = get_signal_formats(header)\n",
        "    adc_gains = get_adc_gains(header)\n",
        "    baselines = get_baselines(header)\n",
        "    signal_units = get_signal_units(header)\n",
        "    signal_names = get_signal_names(header)\n",
        "\n",
        "    if all(signal_format == '16' for signal_format in signal_formats):\n",
        "        signal = np.clip(signal, -2**15 + 1, 2**15 - 1)\n",
        "        signal = np.asarray(signal, dtype=np.int16)\n",
        "    else:\n",
        "        signal_format_string = ', '.join(sorted(set(signal_formats)))\n",
        "        raise NotImplementedError(f'{signal_format_string} not implemented')\n",
        "\n",
        "    import wfdb\n",
        "    wfdb.wrsamp(record, fs=sampling_frequency, units=signal_units, sig_name=signal_names, \\\n",
        "                d_signal=signal, fmt=signal_formats, adc_gain=adc_gains, baseline=baselines, comments=comments, \\\n",
        "                write_dir=path)"
      ],
      "metadata": {
        "id": "ZKQduNYhvFUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 예시 신호 생성\n",
        "    sampling_frequency = 1000  # 샘플링 주파수\n",
        "    duration = 10  # 신호의 길이 (초)\n",
        "    num_samples = sampling_frequency * duration\n",
        "    time = np.linspace(0, duration, num_samples)\n",
        "    amplitude = 0.5 * np.sin(2 * np.pi * 10 * time)  # 주파수가 10 Hz인 사인 파형 생성\n",
        "\n",
        "    # 레코드 파일 경로\n",
        "    record_path = \"example_record\"\n",
        "\n",
        "    # 신호 저장\n",
        "    save_signal(record_path, amplitude)"
      ],
      "metadata": {
        "id": "uXgkYtq3vFJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_signals"
      ],
      "metadata": {
        "id": "x7Lt7cUEwj4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the signal(s) for a record.\n",
        "def save_signals(record, signals):\n",
        "    save_signal(record, signals)"
      ],
      "metadata": {
        "id": "aHaYb3jOvFBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_dx\n",
        "- 레코드에 대한 진단 클래스를 저장하는 역할\n",
        "  - record: 진단 클래스를 저장할 레코드 파일의 경로\n",
        "  - dx: 저장할 진단 클래스(들)을 담은 리스트"
      ],
      "metadata": {
        "id": "gqgby1DGwp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dx class(es) for a record.\n",
        "def save_dx(record, dx):\n",
        "    header_file = get_header_file(record)\n",
        "    header = load_text(header_file)\n",
        "    header += '#Dx: ' + ', '.join(dx) + '\\n'\n",
        "    save_text(header_file, header)\n",
        "    return header"
      ],
      "metadata": {
        "id": "VVWLkWYivE0p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 레코드 파일 경로\n",
        "    record_path = \"example_record.txt\"\n",
        "\n",
        "    # 저장할 진단 클래스들\n",
        "    diagnosis_classes = [\"A\", \"B\", \"C\"]\n",
        "\n",
        "    # 진단 클래스 저장\n",
        "    updated_header = save_dx(record_path, diagnosis_classes)\n",
        "    print(\"수정된 헤더 정보:\")\n",
        "    print(updated_header)"
      ],
      "metadata": {
        "id": "8476jLATwlQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Challenge functions"
      ],
      "metadata": {
        "id": "QXSHdjFdxxJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_text\n",
        "- 텍스트 파일의 내용을 문자열로 불러옴"
      ],
      "metadata": {
        "id": "A9Xin8h4x1wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a text file as a string.\n",
        "def load_text(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        string = f.read()\n",
        "    return string"
      ],
      "metadata": {
        "id": "6uW3Hoi4xRPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 불러올 텍스트 파일 경로\n",
        "    file_path = \"example.txt\"\n",
        "\n",
        "    # 텍스트 파일 불러오기\n",
        "    file_content = load_text(file_path)\n",
        "    print(\"불러온 텍스트 파일 내용:\")\n",
        "    print(file_content)"
      ],
      "metadata": {
        "id": "jKigF29TxRHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_text\n",
        "- 문자열을 텍스트 파일로 저장"
      ],
      "metadata": {
        "id": "boxh7ZdNyM20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a string as a text file.\n",
        "def save_text(filename, string):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(string)"
      ],
      "metadata": {
        "id": "b4BoIGmZyHnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 저장할 텍스트 파일 경로\n",
        "    file_path = \"example.txt\"\n",
        "\n",
        "    # 저장할 문자열\n",
        "    content = \"Hello, world!\"\n",
        "\n",
        "    # 텍스트 파일로 문자열 저장\n",
        "    save_text(file_path, content)\n",
        "    print(\"텍스트 파일이 성공적으로 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "6TS0HspNyHi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_variable\n",
        "- 문자열에서 특정 변수의 값을 추출\n",
        "\n",
        "1. 문자열을 줄 단위로 분리합니다.\n",
        "2. 각 줄을 순회하면서 변수 이름으로 시작하는 줄을 찾습니다.\n",
        "3. 해당 줄에서 변수 값을 추출합니다.\n",
        "4. 변수 값을 반환합니다."
      ],
      "metadata": {
        "id": "VpjIXonDyZQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a variable from a string.\n",
        "def get_variable(string, variable_name):\n",
        "    variable = ''\n",
        "    has_variable = False\n",
        "    for l in string.split('\\n'):\n",
        "        if l.startswith(variable_name):\n",
        "            variable = l[len(variable_name):].strip()\n",
        "            has_variable = True\n",
        "    return variable, has_variable"
      ],
      "metadata": {
        "id": "iNFMP1STyTq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 문자열\n",
        "    text = \"x = 10\\ny = 20\\nz = 30\"\n",
        "\n",
        "    # 가져올 변수의 이름\n",
        "    variable_name = \"y\"\n",
        "\n",
        "    # 변수 값 가져오기\n",
        "    variable_value, exists = get_variable(text, variable_name)\n",
        "\n",
        "    # 결과 출력\n",
        "    if exists:\n",
        "        print(f\"변수 '{variable_name}'의 값: {variable_value}\")\n",
        "    else:\n",
        "        print(f\"변수 '{variable_name}'가 존재하지 않습니다.\")"
      ],
      "metadata": {
        "id": "U-v0m4gcyToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_variables\n",
        "- 문자열에서 특정 변수들의 값을 추출"
      ],
      "metadata": {
        "id": "La0-fvmlzOrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get variables from a string.\n",
        "def get_variables(string, variable_name, sep=','):\n",
        "    variables = list()\n",
        "    has_variable = False\n",
        "    for l in string.split('\\n'):\n",
        "        if l.startswith(variable_name):\n",
        "            variables += [variable.strip() for variable in l[len(variable_name):].strip().split(sep)]\n",
        "            has_variable = True\n",
        "    return variables, has_variable"
      ],
      "metadata": {
        "id": "jTafKjP6zN9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_signal_files_from_header\n",
        "-  문자열에서 신호 파일의 리스트를 추출\n",
        "\n",
        "1. 문자열을 줄 단위로 분리합니다.\n",
        "2. 각 줄을 순회하면서 신호 파일을 찾습니다.\n",
        "3. 신호 파일을 추출하고, 이전에 발견한 신호 파일과 중복되지 않는 경우에만 리스트에 추가합니다.\n",
        "4. 신호 파일 리스트를 반환합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "F8Osy51Hy0nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the signal file(s) from a header or a similar string.\n",
        "def get_signal_files_from_header(string):\n",
        "    signal_files = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        arrs = [arr.strip() for arr in l.split(' ')]\n",
        "        if i==0 and not l.startswith('#'):\n",
        "            num_channels = int(arrs[1])\n",
        "        elif i<=num_channels and not l.startswith('#'):\n",
        "            signal_file = arrs[0]\n",
        "            if signal_file not in signal_files:\n",
        "                signal_files.append(signal_file)\n",
        "        else:\n",
        "            break\n",
        "    return signal_files"
      ],
      "metadata": {
        "id": "XoJI1MAXyHbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 문자열\n",
        "    header_string = \"# Record 1\\nsignal1.dat 16\\nsignal2.dat 16\\n# End\"\n",
        "\n",
        "    # 신호 파일 리스트 가져오기\n",
        "    signal_files = get_signal_files_from_header(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"신호 파일 리스트:\", signal_files)"
      ],
      "metadata": {
        "id": "ZYO_qmP3yvL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_image_files_from_header\n",
        "-  문자열에서 이미지 파일의 리스트를 추출\n",
        "\n",
        "1. get_variables 함수를 사용하여 문자열에서 이미지 파일들을 가져옵니다. 이 함수는 변수 값을 가져오는 역할을 합니다.\n",
        "2. 가져온 이미지 파일 리스트가 비어있는지 확인합니다.\n",
        "3. 이미지 파일 리스트를 반환합니다. 이미지 파일이 없다면 예외를 발생시킵니다."
      ],
      "metadata": {
        "id": "4UTZVqARzvSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the image file(s) from a header or a similar string.\n",
        "def get_image_files_from_header(string):\n",
        "    images, has_image = get_variables(string, '#Image:')\n",
        "    if not has_image:\n",
        "        raise Exception('No images available: did you forget to generate or include the images?')\n",
        "    return images"
      ],
      "metadata": {
        "id": "zvdEVV8wyvEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 문자열\n",
        "    header_string = \"#Image: image1.jpg, image2.png\"\n",
        "\n",
        "    # 이미지 파일 리스트 가져오기\n",
        "    image_files = get_image_files_from_header(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"이미지 파일 리스트:\", image_files)"
      ],
      "metadata": {
        "id": "1r1AzQ5Lztk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_dxs_from_header\n",
        "- 문자열에서 진단 클래스의 리스트 추출"
      ],
      "metadata": {
        "id": "8lt1uJFi0JP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dx class(es) from a header or a similar string.\n",
        "def get_dxs_from_header(string):\n",
        "    dxs, has_dx = get_variables(string, '#Dx:')\n",
        "    if not has_dx:\n",
        "        raise Exception('No dx classes available: are you trying to load the classes from the held-out dataset, or did you forget to prepare the data to include the classes?')\n",
        "    return dxs"
      ],
      "metadata": {
        "id": "65ildN9eztfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 문자열\n",
        "    header_string = \"#Dx: Class1, Class2, Class3\"\n",
        "\n",
        "    # 진단 클래스 리스트 가져오기\n",
        "    dx_classes = get_dxs_from_header(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"진단 클래스 리스트:\", dx_classes)"
      ],
      "metadata": {
        "id": "RWMpIXd80CYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_header_file\n",
        "- 레코드 파일에 해당하는 헤더 파일의 경로를 가져옴\n",
        "\n",
        "1. 만약 입력된 레코드 파일의 확장자가 '.hea'로 끝나지 않는다면, 확장자를 추가하여 헤더 파일의 경로를 생성합니다.\n",
        "2. 이미 '.hea'로 끝나는 경우, 입력된 경로를 그대로 사용합니다.\n",
        "3. 생성된 헤더 파일의 경로를 반환합니다."
      ],
      "metadata": {
        "id": "y-4VPsZR0jmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the header file for a record.\n",
        "def get_header_file(record):\n",
        "    if not record.endswith('.hea'):\n",
        "        header_file = record + '.hea'\n",
        "    else:\n",
        "        header_file = record\n",
        "    return header_file"
      ],
      "metadata": {
        "id": "vq_FkzIS0eiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 레코드 파일 경로\n",
        "    record_path = \"example_record.dat\"\n",
        "\n",
        "    # 헤더 파일 경로 가져오기\n",
        "    header_file_path = get_header_file(record_path)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"헤더 파일 경로:\", header_file_path)"
      ],
      "metadata": {
        "id": "wO--A7nJ0edz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_signal_files\n",
        "-  레코드에 대한 신호 파일을 가져옴"
      ],
      "metadata": {
        "id": "pswzlgxM01Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the signal file(s) for a record.\n",
        "def get_signal_files(record):\n",
        "    header_file = get_header_file(record)\n",
        "    header = load_text(header_file)\n",
        "    signal_files = get_signal_files_from_header(header)\n",
        "    return signal_files"
      ],
      "metadata": {
        "id": "4RaDqUlJ0wiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 레코드 파일 경로\n",
        "    record_path = \"example_record.dat\"\n",
        "\n",
        "    # 신호 파일 리스트 가져오기\n",
        "    signal_files = get_signal_files(record_path)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"신호 파일 리스트:\", signal_files)"
      ],
      "metadata": {
        "id": "SjBAOLFN0wc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_image_files\n",
        "- 레코드 파일에 해당하는 이미지 파일의 리스트를 가져옴"
      ],
      "metadata": {
        "id": "G5TL0Ch21QAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the image file(s) for a record.\n",
        "def get_image_files(record):\n",
        "    header_file = get_header_file(record)\n",
        "    header = load_text(header_file)\n",
        "    image_files = get_image_files_from_header(header)\n",
        "    return image_files"
      ],
      "metadata": {
        "id": "ecqBbJmQ0CPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 레코드 파일 경로\n",
        "    record_path = \"example_record.dat\"\n",
        "\n",
        "    # 이미지 파일 리스트 가져오기\n",
        "    image_files = get_image_files(record_path)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"이미지 파일 리스트:\", image_files)"
      ],
      "metadata": {
        "id": "zcUTBUxF1LYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WFDB functions\n",
        "- WFDB는 \"Waveform Database\"의 약어\n",
        "- WFDB는 생체 신호 데이터를 읽고 쓰며, 신호 처리 및 분석에 사용하는 다양한 도구와 라이브러리를 제공합니다.\n",
        "- 주로 심전도, 심박동, 혈압 및 호흡 데이터와 같은 생체 의학적인 신호를 다루는 데 사용됩니다."
      ],
      "metadata": {
        "id": "Lz2gwHw-1ZfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_record_name\n",
        "- 헤더 파일에서 레코드 이름을 가져오는 역할\n",
        "\n",
        "1. 문자열을 줄 단위로 분리하고, 첫 번째 줄을 선택\n",
        "2. 선택된 줄을 공백을 기준으로 나눈 후, 첫 번째 요소를 선택\n",
        "3. 선택된 요소를 '/'를 기준으로 나눈 후, 첫 번째 요소를 선택\n",
        "4. 최종적으로 선택된 요소의 좌우 공백을 제거하고, 레코드 이름을 반환"
      ],
      "metadata": {
        "id": "LnrHfzjB17g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the record name from a header file.\n",
        "def get_record_name(string):\n",
        "    value = string.split('\\n')[0].split(' ')[0].split('/')[0].strip()\n",
        "    return value"
      ],
      "metadata": {
        "id": "VR94zI9K1WVO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 헤더 파일 내용\n",
        "    header_string = \"# Record 1\\nsignal1.dat 16\\nsignal2.dat 16\\n# End\"\n",
        "\n",
        "    # 레코드 이름 추출\n",
        "    record_name = get_record_name(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"레코드 이름:\", record_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWvuJfK02NIv",
        "outputId": "e4c7c6ce-364a-4b30-93e3-185904d25208"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "레코드 이름: #\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_num_signals\n",
        "- 헤더 파일에서 신호의 수를 추출하는 방법"
      ],
      "metadata": {
        "id": "sANABgkI2S9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of signals from a header file.\n",
        "def get_num_signals(string):\n",
        "    value = string.split('\\n')[0].split(' ')[1].strip()\n",
        "    if is_integer(value):\n",
        "        value = int(value)\n",
        "    else:\n",
        "        value = None\n",
        "    return value"
      ],
      "metadata": {
        "id": "O1DBKboU2ObP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 헤더 파일 내용\n",
        "    header_string = \"# Record 1\\n2\\nsignal1.dat 16\\nsignal2.dat 16\\n# End\"\n",
        "\n",
        "    # 신호의 수 추출\n",
        "    num_signals = get_num_signals(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"신호의 수:\", num_signals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugHh8EbD2OVZ",
        "outputId": "bac108df-7556-42e5-b482-24e9fdcc739a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "신호의 수: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_sampling_frequency\n",
        "- 헤더 파일에서 샘플링 주파수를 추출"
      ],
      "metadata": {
        "id": "SaeKUh824M6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the sampling frequency from a header file.\n",
        "def get_sampling_frequency(string):\n",
        "    value = string.split('\\n')[0].split(' ')[2].split('/')[0].strip()\n",
        "    if is_number(value):\n",
        "        value = float(value)\n",
        "    else:\n",
        "        value = None\n",
        "    return value"
      ],
      "metadata": {
        "id": "iLJWrj_B4MKA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 헤더 파일 내용\n",
        "    header_string = \"# Record 1\\n2\\n1000/1\\nsignal1.dat 16\\nsignal2.dat 16\\n# End\"\n",
        "\n",
        "    # 샘플링 주파수 추출\n",
        "    sampling_frequency = get_sampling_frequency(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"샘플링 주파수:\", sampling_frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agAMBKoh4MB_",
        "outputId": "6df33c67-f9d3-4a43-984c-6ab35c229f57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플링 주파수: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_num_samples\n",
        "- 헤더 파일에서 샘플의 수를 추출"
      ],
      "metadata": {
        "id": "HN-kjb7Z41zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of samples from a header file.\n",
        "def get_num_samples(string):\n",
        "    value = string.split('\\n')[0].split(' ')[3].strip()\n",
        "    if is_integer(value):\n",
        "        value = int(value)\n",
        "    else:\n",
        "        value = None\n",
        "    return value"
      ],
      "metadata": {
        "id": "QwTTjCwa4L69"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 헤더 파일 내용\n",
        "    header_string = \"# Record 1\\n2\\n1000/1\\n5000\\nsignal1.dat 16\\nsignal2.dat 16\\n# End\"\n",
        "\n",
        "    # 샘플의 수 추출\n",
        "    num_samples = get_num_samples(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"샘플의 수:\", num_samples)"
      ],
      "metadata": {
        "id": "Z6uSAzwQ4vOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_signal_formats\n",
        "- 헤더 파일에서 신호 형식을 추출\n",
        "\n",
        "1. get_num_signals 함수를 사용하여 헤더 파일에 포함된 신호의 수를 가져옵니다.\n",
        "2. 문자열을 줄 단위로 분리한 후, 각 줄에서 신호 형식을 추출합니다.\n",
        "3. 각 줄에서 추출한 신호 형식을 리스트에 추가합니다."
      ],
      "metadata": {
        "id": "wS_Hj1ow5F_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the signal formats from a header file.\n",
        "def get_signal_formats(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[1]\n",
        "            if 'x' in field:\n",
        "                field = field.split('x')[0]\n",
        "            if ':' in field:\n",
        "                field = field.split(':')[0]\n",
        "            if '+' in field:\n",
        "                field = field.split('+')[0]\n",
        "            value = field\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "tbwIwdDm4vLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 헤더 파일 내용\n",
        "    header_string = \"# Record 1\\n2\\n1000/1\\n5000\\nsignal1.dat 16\\nsignal2.dat 24\\n# End\"\n",
        "\n",
        "    # 신호 형식 리스트 가져오기\n",
        "    signal_formats = get_signal_formats(header_string)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"신호 형식 리스트:\", signal_formats)"
      ],
      "metadata": {
        "id": "K74TRz7J4-us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_adc_gains\n",
        "- 헤더 파일에서 ADC(Analog-to-Digital Converter) 게인을 추출"
      ],
      "metadata": {
        "id": "nirdWJFO5jg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ADC gains from a header file.\n",
        "def get_adc_gains(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[2]\n",
        "            if '/' in field:\n",
        "                field = field.split('/')[0]\n",
        "            if '(' in field and ')' in field:\n",
        "                field = field.split('(')[0]\n",
        "            value = float(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "M968HkJT5e4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mYUd_zF5ezQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_baselines\n",
        "- 헤더 파일에서 베이스라인을 추출"
      ],
      "metadata": {
        "id": "nqWC_AOW50We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the baselines from a header file.\n",
        "def get_baselines(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[2]\n",
        "            if '/' in field:\n",
        "                field = field.split('/')[0]\n",
        "            if '(' in field and ')' in field:\n",
        "                field = field.split('(')[1].split(')')[0]\n",
        "            else:\n",
        "                field = get_adc_zeros(string)[i-1]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "G1E-DFrG5w2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_signal_units\n",
        "- 헤더 파일에서 신호의 단위를 추출"
      ],
      "metadata": {
        "id": "cvVDlMQ06KSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the signal units from a header file.\n",
        "def get_signal_units(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[2]\n",
        "            if '/' in field:\n",
        "                value = field.split('/')[1]\n",
        "            else:\n",
        "                value = 'mV'\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "VM6sdg7V5wyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_adc_resolutions\n",
        "- 헤더 파일에서 ADC 해상도를 추출"
      ],
      "metadata": {
        "id": "qgIggHWL6TVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ADC resolutions from a header file.\n",
        "def get_adc_resolutions(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[3]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "jmHrIPAM6PDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_adc_zeros\n",
        "- 헤더 파일에서 ADC 제로 값을 추출"
      ],
      "metadata": {
        "id": "AmmT5Zkr6cyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ADC zeros from a header file.\n",
        "def get_adc_zeros(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[4]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "qPxlsbb_6O8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_initial_values\n",
        "- 헤더 파일에서 신호의 초기값을 추출"
      ],
      "metadata": {
        "id": "tXDvODd-6lMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial values of a signal from a header file.\n",
        "def get_initial_values(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[5]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "ycixPiki6iGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_checksums\n",
        "- 헤더 파일에서 신호의 체크섬을 추출\n",
        "  - 신호의 checksum은 해당 신호의 데이터 무결성을 검증하기 위한 값입니다.\n",
        "  - 일반적으로 데이터 전송 중에 사용되며, 데이터의 일부 또는 전체에 대한 해시를 계산하여 생성됩니다.\n",
        "  - 수신 측에서는 수신된 데이터의 checksum을 계산하고, 이를 송신 측에서 전송한 checksum과 비교하여 데이터의 손상 여부를 확인합니다.\n",
        "  - 이를 통해 데이터 전송 중 발생한 오류를 감지할 수 있습니다."
      ],
      "metadata": {
        "id": "m504EZm663kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the checksums of a signal from a header file.\n",
        "def get_checksums(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[6]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "q0BNLjog6iDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_block_sizes\n",
        "- 헤더 파일에서 신호의 블록 크기를 추출"
      ],
      "metadata": {
        "id": "HLStK5Te7Www"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the block sizes of a signal from a header file.\n",
        "def get_block_sizes(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            field = l.split(' ')[7]\n",
        "            value = int(field)\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "jbiWfqEX7WJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_signal_names"
      ],
      "metadata": {
        "id": "8zpAM-0z7e2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the signal names from a header file.\n",
        "def get_signal_names(string):\n",
        "    num_signals = get_num_signals(string)\n",
        "    values = list()\n",
        "    for i, l in enumerate(string.split('\\n')):\n",
        "        if 1 <= i <= num_signals:\n",
        "            value = l.split(' ')[8]\n",
        "            values.append(value)\n",
        "    return values"
      ],
      "metadata": {
        "id": "VrBV7SX07eNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Evaluation functions"
      ],
      "metadata": {
        "id": "0cLurUnL7vcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluation functions\n",
        "\n",
        "# Construct the binary one-vs-rest confusion matrices, where the columns are the expert labels and the rows are the classifier\n",
        "# for the given classes.\n",
        "def compute_one_vs_rest_confusion_matrix(labels, outputs, classes):\n",
        "    assert np.shape(labels) == np.shape(outputs)\n",
        "\n",
        "    num_instances = len(labels)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    A = np.zeros((num_classes, 2, 2))\n",
        "    for i in range(num_instances):\n",
        "        for j in range(num_classes):\n",
        "            if labels[i, j] == 1 and outputs[i, j] == 1: # TP\n",
        "                A[j, 0, 0] += 1\n",
        "            elif labels[i, j] == 0 and outputs[i, j] == 1: # FP\n",
        "                A[j, 0, 1] += 1\n",
        "            elif labels[i, j] == 1 and outputs[i, j] == 0: # FN\n",
        "                A[j, 1, 0] += 1\n",
        "            elif labels[i, j] == 0 and outputs[i, j] == 0: # TN\n",
        "                A[j, 1, 1] += 1\n",
        "\n",
        "    return A\n",
        "\n",
        "# Compute macro F-measure.\n",
        "def compute_f_measure(labels, outputs):\n",
        "    # Compute confusion matrix.\n",
        "    classes = sorted(set.union(*map(set, labels)))\n",
        "    labels = compute_one_hot_encoding(labels, classes)\n",
        "    outputs = compute_one_hot_encoding(outputs, classes)\n",
        "    A = compute_one_vs_rest_confusion_matrix(labels, outputs, classes)\n",
        "\n",
        "    num_classes = len(classes)\n",
        "    per_class_f_measure = np.zeros(num_classes)\n",
        "    for k in range(num_classes):\n",
        "        tp, fp, fn, tn = A[k, 0, 0], A[k, 0, 1], A[k, 1, 0], A[k, 1, 1]\n",
        "        if 2 * tp + fp + fn > 0:\n",
        "            per_class_f_measure[k] = float(2 * tp) / float(2 * tp + fp + fn)\n",
        "        else:\n",
        "            per_class_f_measure[k] = float('nan')\n",
        "\n",
        "    if np.any(np.isfinite(per_class_f_measure)):\n",
        "        macro_f_measure = np.nanmean(per_class_f_measure)\n",
        "    else:\n",
        "        macro_f_measure = float('nan')\n",
        "\n",
        "    return macro_f_measure, per_class_f_measure, classes\n",
        "\n",
        "# Reorder channels in signal.\n",
        "def reorder_signal(input_signal, input_channels, output_channels):\n",
        "    # Do not allow repeated channels with potentially different values in a signal.\n",
        "    assert(len(set(input_channels)) == len(input_channels))\n",
        "    assert(len(set(output_channels)) == len(output_channels))\n",
        "\n",
        "    if input_channels == output_channels:\n",
        "        output_signal = input_signal\n",
        "    else:\n",
        "        input_channels = [channel.strip().casefold() for channel in input_channels]\n",
        "        output_channels = [channel.strip().casefold() for channel in output_channels]\n",
        "\n",
        "        input_signal = np.asarray(input_signal)\n",
        "        num_samples = np.shape(input_signal)[0]\n",
        "        num_channels = len(output_channels)\n",
        "        data_type = input_signal.dtype\n",
        "        output_signal = np.zeros((num_samples, num_channels), dtype=data_type)\n",
        "\n",
        "        for i, output_channel in enumerate(output_channels):\n",
        "            for j, input_channel in enumerate(input_channels):\n",
        "                if input_channel == output_channel:\n",
        "                    output_signal[:, i] = input_signal[:, j]\n",
        "\n",
        "    return output_signal\n",
        "\n",
        "# Pad or truncate signal.\n",
        "def trim_signal(input_signal, num_samples_trimmed):\n",
        "    input_signal = np.asarray(input_signal)\n",
        "    num_samples, num_channels = np.shape(input_signal)\n",
        "    data_type = input_signal.dtype\n",
        "\n",
        "    if num_samples == num_samples_trimmed:\n",
        "        output_signal = input_signal\n",
        "    else:\n",
        "        output_signal = np.zeros((num_samples_trimmed, num_channels), dtype=data_type)\n",
        "        if num_samples < num_samples_trimmed: # Zero-pad the signals.\n",
        "            output_signal[:num_samples, :] = input_signal\n",
        "        else: # Truncate the signals.\n",
        "            output_signal = input_signal[:num_samples_trimmed, :]\n",
        "\n",
        "    return output_signal\n",
        "\n",
        "# Compute SNR.\n",
        "def compute_snr(label_signal, output_signal):\n",
        "    label_signal = np.asarray(label_signal)\n",
        "    output_signal = np.asarray(output_signal)\n",
        "\n",
        "    assert(label_signal.ndim == output_signal.ndim == 1)\n",
        "    assert(np.size(label_signal) == np.size(output_signal))\n",
        "\n",
        "    idx_finite_signal = np.isfinite(label_signal)\n",
        "    label_signal = label_signal[idx_finite_signal]\n",
        "    output_signal = output_signal[idx_finite_signal]\n",
        "\n",
        "    idx_nan_signal = np.isnan(output_signal)\n",
        "    output_signal[idx_nan_signal] = 0\n",
        "\n",
        "    noise_signal = output_signal - label_signal\n",
        "\n",
        "    x = np.sum(label_signal**2)\n",
        "    y = np.sum(noise_signal**2)\n",
        "\n",
        "    if x > 0 and y > 0:\n",
        "        snr = 10 * np.log10(x / y)\n",
        "    elif x > 0 and y == 0:\n",
        "        snr = float('inf')\n",
        "    else:\n",
        "        snr = float('nan')\n",
        "\n",
        "    return snr\n",
        "\n",
        "# Compute the mean signal power to median noise power metric.\n",
        "def compute_snr_median(label_signal, output_signal):\n",
        "    label_signal = np.asarray(label_signal)\n",
        "    output_signal = np.asarray(output_signal)\n",
        "\n",
        "    assert(label_signal.ndim == output_signal.ndim == 1)\n",
        "    assert(np.size(label_signal) == np.size(output_signal))\n",
        "\n",
        "    idx_finite_signal = np.isfinite(label_signal)\n",
        "    label_signal = label_signal[idx_finite_signal]\n",
        "    output_signal = output_signal[idx_finite_signal]\n",
        "\n",
        "    idx_nan_signal = np.isnan(output_signal)\n",
        "    output_signal[idx_nan_signal] = 0\n",
        "\n",
        "    noise_signal = output_signal - label_signal\n",
        "\n",
        "    x = np.mean(label_signal**2)\n",
        "    y = np.median(noise_signal**2)\n",
        "\n",
        "    if y > 0:\n",
        "        snr = 10 * np.log10(x / y)\n",
        "    else:\n",
        "        snr = float('inf')\n",
        "\n",
        "    return snr\n",
        "\n",
        "# Compute a metric inspired by the Kolmogorov-Smirnov test statistic.\n",
        "def compute_ks_metric(label_signal, output_signal):\n",
        "    label_signal = np.asarray(label_signal)\n",
        "    output_signal = np.asarray(output_signal)\n",
        "\n",
        "    assert(label_signal.ndim == output_signal.ndim == 1)\n",
        "    assert(np.size(label_signal) == np.size(output_signal))\n",
        "\n",
        "    idx_finite_signal = np.isfinite(label_signal)\n",
        "    label_signal = label_signal[idx_finite_signal]\n",
        "    output_signal = output_signal[idx_finite_signal]\n",
        "\n",
        "    idx_nan_signal = np.isnan(output_signal)\n",
        "    output_signal[idx_nan_signal] = 0\n",
        "\n",
        "    label_signal_cdf = np.cumsum(np.abs(label_signal))\n",
        "    output_signal_cdf = np.cumsum(np.abs(output_signal))\n",
        "\n",
        "    if label_signal_cdf[-1] > 0:\n",
        "        label_signal_cdf = label_signal_cdf / label_signal_cdf[-1]\n",
        "    if output_signal_cdf[-1] > 0:\n",
        "        output_signal_cdf = output_signal_cdf / output_signal_cdf[-1]\n",
        "\n",
        "    goodness_of_fit = 1.0 - np.max(np.abs(label_signal_cdf - output_signal_cdf))\n",
        "\n",
        "    return goodness_of_fit\n",
        "\n",
        "# Compute the adaptive signed correlation index (ASCI) metric.\n",
        "def compute_asci_metric(label_signal, output_signal, beta=0.05):\n",
        "    label_signal = np.asarray(label_signal)\n",
        "    output_signal = np.asarray(output_signal)\n",
        "\n",
        "    assert(label_signal.ndim == output_signal.ndim == 1)\n",
        "    assert(np.size(label_signal) == np.size(output_signal))\n",
        "\n",
        "    idx_finite_signal = np.isfinite(label_signal)\n",
        "    label_signal = label_signal[idx_finite_signal]\n",
        "    output_signal = output_signal[idx_finite_signal]\n",
        "\n",
        "    idx_nan_signal = np.isnan(output_signal)\n",
        "    output_signal[idx_nan_signal] = 0\n",
        "\n",
        "    if beta <= 0 or beta > 1:\n",
        "        raise ValueError('The beta value should be greater than 0 and less than or equal to 1.')\n",
        "\n",
        "    threshold = beta * np.std(label_signal)\n",
        "\n",
        "    noise_signal = np.abs(label_signal - output_signal)\n",
        "\n",
        "    discrete_noise = np.zeros_like(noise_signal)\n",
        "    discrete_noise[noise_signal <= threshold] = 1\n",
        "    discrete_noise[noise_signal > threshold] = -1\n",
        "\n",
        "    asci = np.mean(discrete_noise)\n",
        "\n",
        "    return asci\n",
        "\n",
        "# Compute a weighted absolute difference metric.\n",
        "def compute_weighted_absolute_difference(label_signal, output_signal, sampling_frequency):\n",
        "    label_signal = np.asarray(label_signal)\n",
        "    output_signal = np.asarray(output_signal)\n",
        "\n",
        "    assert(label_signal.ndim == output_signal.ndim == 1)\n",
        "    assert(np.size(label_signal) == np.size(output_signal))\n",
        "\n",
        "    idx_finite_signal = np.isfinite(label_signal)\n",
        "    label_signal = label_signal[idx_finite_signal]\n",
        "    output_signal = output_signal[idx_finite_signal]\n",
        "\n",
        "    idx_nan_signal = np.isnan(output_signal)\n",
        "    output_signal[idx_nan_signal] = 0\n",
        "\n",
        "    from scipy.signal import filtfilt\n",
        "\n",
        "    m = round(0.1 * sampling_frequency)\n",
        "    w = filtfilt(np.ones(m), m, label_signal, method='gust')\n",
        "    w = 1 - 0.5/np.max(w) * w\n",
        "    n = np.sum(w)\n",
        "\n",
        "    weighted_absolute_difference_metric = np.sum(np.abs(label_signal-output_signal) * w)/n\n",
        "\n",
        "    return weighted_absolute_difference_metric\n",
        "\n",
        "### Other helper functions\n",
        "\n",
        "# Check if a variable is a number or represents a number.\n",
        "def is_number(x):\n",
        "    try:\n",
        "        float(x)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "# Check if a variable is an integer or represents an integer.\n",
        "def is_integer(x):\n",
        "    if is_number(x):\n",
        "        return float(x).is_integer()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Check if a variable is a finite number or represents a finite number.\n",
        "def is_finite_number(x):\n",
        "    if is_number(x):\n",
        "        return np.isfinite(float(x))\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Check if a variable is a NaN (not a number) or represents a NaN.\n",
        "def is_nan(x):\n",
        "    if is_number(x):\n",
        "        return np.isnan(float(x))\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Cast a value to an integer if an integer, a float if a non-integer float, and an unknown value otherwise.\n",
        "def cast_int_float_unknown(x):\n",
        "    if is_integer(x):\n",
        "        x = int(x)\n",
        "    elif is_finite_number(x):\n",
        "        x = float(x)\n",
        "    elif is_number(x):\n",
        "        x = 'Unknown'\n",
        "    else:\n",
        "        raise NotImplementedError(f'Unable to cast {x}.')\n",
        "    return x\n",
        "\n",
        "# Construct the one-hot encoding of data for the given classes.\n",
        "def compute_one_hot_encoding(data, classes):\n",
        "    num_instances = len(data)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    one_hot_encoding = np.zeros((num_instances, num_classes), dtype=np.bool_)\n",
        "    unencoded_data = list()\n",
        "    for i, x in enumerate(data):\n",
        "        for y in x:\n",
        "            for j, z in enumerate(classes):\n",
        "                if (y == z) or (is_nan(y) and is_nan(z)):\n",
        "                    one_hot_encoding[i, j] = 1\n",
        "\n",
        "    return one_hot_encoding"
      ],
      "metadata": {
        "id": "6WUUU-rj1LVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lISmvWrEprCl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0_uGnEqp0ce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}